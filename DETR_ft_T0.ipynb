{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detr_dataset import COCODataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define DataLoader function\n",
    "def load_coco_dataset(json_path, image_dir):\n",
    "    print(\"Loading COCO JSON file...\")\n",
    "    # Read COCO JSON file\n",
    "    with open(json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    print(\"Loaded COCO JSON file successfully.\")\n",
    "\n",
    "    # Extract annotations and images information\n",
    "    annotations = coco_data['annotations']\n",
    "    images = {img['id']: img for img in coco_data['images']}\n",
    "    print(f\"Found {len(images)} images and {len(annotations)} annotations.\")\n",
    "\n",
    "    # Translate category names to English\n",
    "    for category in coco_data['categories']:\n",
    "        if 'name' in category:\n",
    "            # Add your specific translation logic here if necessary\n",
    "            category['name'] = category['name'].lower()  # Example: convert to lowercase\n",
    "\n",
    "    # Construct dataset list\n",
    "    dataset = []\n",
    "    for i, ann in enumerate(annotations):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing annotation {i}/{len(annotations)}...\")\n",
    "        img_info = images[ann['image_id']]\n",
    "        \n",
    "        # Get image-related information\n",
    "        img_path = os.path.join(image_dir, img_info['file_name'])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Correct area calculation from bbox\n",
    "        bbox = ann['bbox']  # Format: [x_min, y_min, width, height]\n",
    "        calculated_area = bbox[2] * bbox[3]\n",
    "\n",
    "        # Construct object information\n",
    "        obj_info = {\n",
    "            'id': ann['category_id'],\n",
    "            'area': calculated_area,  # Use calculated bbox area\n",
    "            'bbox': bbox,\n",
    "            'category': coco_data['categories'][ann['category_id'] - 1]['name']\n",
    "        }\n",
    "\n",
    "        # If the image already exists, append the corresponding object\n",
    "        existing = next((item for item in dataset if item['image_id'] == img_info['id']), None)\n",
    "        if existing:\n",
    "            existing['objects']['id'].append(obj_info['id'])\n",
    "            existing['objects']['area'].append(obj_info['area'])\n",
    "            existing['objects']['bbox'].append(obj_info['bbox'])\n",
    "            existing['objects']['category'].append(obj_info['category'])\n",
    "        else:\n",
    "            dataset.append({\n",
    "                'image': image,\n",
    "                'image_id': img_info['id'],\n",
    "                'width': img_info['width'],\n",
    "                'height': img_info['height'],\n",
    "                'objects': {\n",
    "                    'id': [obj_info['id']],\n",
    "                    'area': [obj_info['area']],\n",
    "                    'bbox': [obj_info['bbox']],\n",
    "                    'category': [obj_info['category']]\n",
    "                }\n",
    "            })\n",
    "\n",
    "    print(\"Splitting dataset into train and test...\")\n",
    "    # Split the dataset into train and test\n",
    "    train_data, test_data = train_test_split(dataset, test_size=0.15, random_state=42)\n",
    "\n",
    "    print(\"Formatting datasets...\")\n",
    "    # Format the data using Hugging Face Dataset\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "    print(\"Datasets formatted successfully.\")\n",
    "    return DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "# Example usage\n",
    "json_path = \"./0115_T0_dataset_coco/result.json\"\n",
    "image_dir = \"./0115_T0_dataset_coco/images\"\n",
    "print(\"Starting dataset loading...\")\n",
    "dataset = load_coco_dataset(json_path, image_dir)\n",
    "print(\"Dataset loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataset structure\n",
    "#print(\"Dataset structure:\")\n",
    "#print(dataset)\n",
    "\n",
    "print(\"Sample train data:\")\n",
    "dataset['train'][0]\n",
    "\n",
    "#print(\"Sample test data:\")\n",
    "#dataset['test'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, extract out the train and test set\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "def draw_image_from_idx(dataset, idx):\n",
    "    sample = dataset[idx]\n",
    "    image = sample[\"image\"]\n",
    "    annotations = sample[\"objects\"]\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = sample[\"width\"], sample[\"height\"]\n",
    "\n",
    "    for i in range(len(annotations[\"id\"])):\n",
    "        box = annotations[\"bbox\"][i]\n",
    "        class_idx = annotations[\"id\"][i]\n",
    "        x, y, w, h = tuple(box)\n",
    "        if max(box) > 1.0:\n",
    "            x1, y1 = int(x), int(y)\n",
    "            x2, y2 = int(x + w), int(y + h)\n",
    "        else:\n",
    "            x1 = int(x * width)\n",
    "            y1 = int(y * height)\n",
    "            x2 = int((x + w) * width)\n",
    "            y2 = int((y + h) * height)\n",
    "        draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=1)\n",
    "        draw.text((x1, y1), annotations[\"category\"][i], fill=\"white\")\n",
    "    return image\n",
    "\n",
    "\n",
    "draw_image_from_idx(dataset=train_dataset, idx=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_images(dataset, indices):\n",
    "    \"\"\"\n",
    "    Plot images and their annotations.\n",
    "    \"\"\"\n",
    "    num_rows = len(indices) // 3\n",
    "    num_cols = 3\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "\n",
    "        # Draw image\n",
    "        image = draw_image_from_idx(dataset, idx)\n",
    "\n",
    "        # Display image on the corresponding subplot\n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Now use the function to plot images\n",
    "\n",
    "plot_images(train_dataset, range(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"facebook/detr-resnet-50-dc5\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the Dataset\n",
    "\n",
    "Before passing the images to the image_processor, let’s also apply different types of augmentations to the images along with their corresponding bounding boxes.\n",
    "\n",
    "In simple terms, augmentations are some set of random transformations like rotations, resizing etc. These are applied to get more samples and to make the vision model more robust towards different conditions of the image. We will use the albumentations library to achieve this. It let’s you to create random transformations of the images so that your sample size increases for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(700, 600),\n",
    "        albumentations.HorizontalFlip(p=1.0),\n",
    "        albumentations.RandomBrightnessContrast(p=1.0),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we initialize all the transformations, we need to make a function which formats the annotations and returns the a list of annotation with a very specific format.\n",
    "\n",
    "This is because the image_processor expects the annotations to be in the following format: {'image_id': int, 'annotations': List[Dict]}, where each dictionary is a COCO object annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine the image and annotation transformations to do transformations over the whole batch of dataset.\n",
    "Here is the final code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming a batch\n",
    "\n",
    "def transform_aug_ann(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"id\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, all you have to do is apply this preprocessing function to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations for both train and test dataset\n",
    "\n",
    "train_dataset_transformed = train_dataset.with_transform(transform_aug_ann)\n",
    "test_dataset_transformed = test_dataset.with_transform(transform_aug_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_transformed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collate_fn is responsible for taking a list of samples from a dataset and converting them into a batch suitable for model’s input format.\n",
    "\n",
    "In general a DataCollator typically performs tasks such as padding, truncating etc. In a custom collate function, we often define what and how we want to group the data into batches or simply, how to represent each batch.\n",
    "\n",
    "Data collator mainly puts the data together and then preprocesses them. Let’s make our collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "id2label = {0: \"defect\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a DETR Model.\n",
    "\n",
    "So, all the heavy lifting is done so far. Now, all that is left is to assemble each part of the puzzle one by one. Let’s go!\n",
    "\n",
    "The training procedure involves the following steps:\n",
    "\n",
    "1. Loading the base (pre-trained) model with AutoModelForObjectDetection using the same checkpoint as in the preprocessing.\n",
    "\n",
    "2. Defining all the hyperparameters and additional arguments inside TrainingArguments.\n",
    "\n",
    "3. Pass the training arguments inside HuggingFace Trainer, along with the model, dataset and image.\n",
    "\n",
    "4. Call the train() method and fine-tune your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the model from the same checkpoint that you used for the preprocessing, remember to pass the label2id and id2label maps that you created earlier from the dataset’s metadata. Additionally, we specify ignore_mismatched_sizes=True to replace the existing classification head with a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import wandb\n",
    "wandb.init(project=\"my_personal_project\", entity=None)\n",
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"detr-resnet-50-T0_LSR_seg-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    logging_steps=20,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"detr_finetune_experiment\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset_transformed,\n",
    "    eval_dataset=test_dataset_transformed,\n",
    "    tokenizer=image_processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is finished, you can now delete the model, because checkpoints are already uploaded in HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and Inference\n",
    "\n",
    "Now we will try to do inference of our new fine-tuned model. For this tutorial, we will be testing for this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "image = Image.open(\"./0115_T0_dataset_coco/images/0a0526f9-FindDefect1_AECA1AOC2SY014C21_1669_709.bmp\")\n",
    "\n",
    "# make the object detection pipeline\n",
    "\n",
    "obj_detector = pipeline(\n",
    "    \"object-detection\", model=\"./detr-resnet-50-T0_LSR_seg-finetuned/checkpoint-4555\"\n",
    ")\n",
    "results = obj_detector(train_dataset[100][\"image\"])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s make a very simple function to plot the results on our image. We get score, label and corresponding bounding boxes co-ordinates from results, which we will we use to draw in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(image, results, threshold=0.7):\n",
    "    image = Image.fromarray(np.uint8(image))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for result in results:\n",
    "        score = result[\"score\"]\n",
    "        label = result[\"label\"]\n",
    "        box = list(result[\"box\"].values())\n",
    "        if score > threshold:\n",
    "            x, y, x2, y2 = tuple(box)\n",
    "            draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "            draw.text((x, y), label, fill=\"white\")\n",
    "            draw.text(\n",
    "                (x + 0.5, y - 0.5),\n",
    "                text=str(score),\n",
    "                fill=\"green\" if score > 0.7 else \"red\",\n",
    "            )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_results(image, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s club everything together into a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image, pipeline, threshold=0.7):\n",
    "    results = pipeline(image)\n",
    "    return plot_results(image, results, threshold)\n",
    "\n",
    "\n",
    "# Let's test for another test image\n",
    "\n",
    "img = test_dataset[0][\"image\"]\n",
    "predict(img, obj_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def plot_images(dataset, indices):\n",
    "    \"\"\"\n",
    "    Plot images and their annotations.\n",
    "    \"\"\"\n",
    "    num_rows = len(indices) // 3\n",
    "    num_cols = 3\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "    for i, idx in tqdm(enumerate(indices), total=len(indices)):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "\n",
    "        # Draw image\n",
    "        image = predict(dataset[idx][\"image\"], obj_detector)\n",
    "\n",
    "        # Display image on the corresponding subplot\n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_images(test_dataset, range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
